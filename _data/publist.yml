
- title: "Emergence and maintenance of excitability: kinetics over structure"
  image: Emergence.jpg
  description: Alongside studies that extend traditional focus on control-based regulation of structural parameters (channel densities), there is a budding interest in self-organization of kinetic parameters. In this picture, ionic channels are continually forced by activity in-and-out of a pool of states not available for the mechanism of excitability. The process, acting on expressed structure, provides a bed for generation of a spectrum of excitability modes.
  authors: Marom S.
  link:
    url: https://www.scopus.com/record/display.uri?eid=2-s2.0-84979084036&origin=inward&txGid=277a6d426894e83ea4feeff0126764b4
    display:  Current Opinion in Neurobiology (2016)
  highlight: 1
  multiple: 1



- title: "Interaction between duration of activity and time course of recovery from slow inactivation in mammalian brain Na+ channels"
  image: Interaction.jpg
  description: The relationships between activity and availability for activation of voltage gated Na channels were examined using the Xenopus expression system. The main point of this work is that the time constant of recovery from the unavailable (inactivated) pool is related to the duration of previous activation by a power law. These relationships extend from tens of milliseconds to several minutes and are intrinsic to the channel protein
  authors: Toib A. , Lyakhov V. , Marom S.
  link:
    url: https://www.jneurosci.org/content/jneuro/18/5/1893.full.pdf
    display:  Journal of Neuroscience (1998)
  highlight: 0
  multiple: 1
  

- title: "State-dependent inactivation of the Kv3 potassium channel"
  image: dummy.png
  description: Multiple states and scallling of rates
  authors: Marom S. , Dagan D.
  link:
    url: https://www.scopus.com/record/display.uri?eid=2-s2.0-0028136017&origin=inward&txGid=5e526d4cbbf88a1cd14fdeadc99a02c5
    display:  Biophysical Journal (1994)
  highlight: 0
  multiple: 1


- title: "Modeling state-dependent inactivation of membrane currents"
  image: dummy.png
  description: Multiple states and scallling of rates 
  authors: Marom S. , Abbott L.F.
  link:
    url: https://www.scopus.com/record/display.uri?eid=2-s2.0-0028101232&origin=inward&txGid=de40cb1b00b2e258bb6b506984b5a15f 
    display:  Biophysical Journal (1994)
  highlight: 0
  multiple: 1
 

- title: "Dynamics of Excitability over Extended Timescales in Cultured Cortical Neurons"
  image: dummy.png
  description: Critical fluctuations
  authors: Gal A. , Eytan D. , Wallach A. , Sandler M. , Schiller J. , Marom S.
  link:
    url: https://www.scopus.com/record/display.uri?eid=2-s2.0-78649735564&origin=inward&txGid=3e9c222e3b2718e1106a3bdfdfda9830
    display:  Journal of Neuroscience (2010)
  highlight: 0
  critical: 1
  

- title: "Self-organized criticality in single-neuron excitability"
  image: self-organized.JPG
  description:  Here, we suggest that neuronal response fluctuations reflect a process that positions the neuron near a transition point that separates excitable and unexcitable phases. This view is supported by the dynamical properties of the system as observed in experiments on isolated cultured cortical neurons, and by a theoretical mapping between the constructs of self-organized criticality and membrane excitability.
  authors: Gal A. , Marom S.
  link:
    url: https://arxiv.org/pdf/1210.7414.pdf
    display: Physical Review E - Statistical, Nonlinear, and Soft Matter Physics (2021)
  highlight: 1
  critical: 1
 

- title: "Entrainment of the intrinsic dynamics of single isolated neurons by natural-like input"
  image: Entrainment.JPG
  description: Neuronal dynamics is intrinsically unstable, producing activity fluctuations that are essentially scale free. Here we experimentally study single cortical neurons, and show that while these scale-free fluctuations are independent of temporal input statistics, they can be entrained by input variation. Response entrainment was found to be maximal when the input itself possesses natural-like, scale-free statistics
  authors: Gal A. , Marom S.
  link:
    url: https://www.scopus.com/record/display.uri?eid=2-s2.0-84877104023&origin=inward&txGid=33b28256c471a1e652f8e506ea921ba5
    display: Journal of Neuroscience
  highlight: 1
  critical: 1
 

- title: "Cellular function given parametric variation in the Hodgkin and Huxley model of excitability"
  image: Cellularfunction.JPG
  description: A theoretical study, showing that although the full Hodgkin–Huxley model is very sensitive to fluctuations that independently occur in its many parameters, the outcome is in fact determined by a simple combinations of these parameters along two physiological dimensions: structural and kinetic (denoted S and K, respectively). The impacts of parametric fluctuations on the dynamics of the system — seemingly complex in the high-dimensional representation of the Hodgkin–Huxley model — are tractable when examined within the S–K plane
  authors: Ori H. , Marder E. , Marom S.
  link:
    url: https://www.pnas.org/doi/full/10.1073/pnas.1808552115
    display:   PNAS (2018)
  highlight: 1
  Parametrization: 1
 
  
- title: "Dynamic clamp constructed phase diagram for the Hodgkin and Huxley model of excitability"
  image: Dynamicclampconstructed.JPG
  description: Parametrization 
  authors: Ori H. , Hazan H. , Marder E. , Marom S.
  link:
    url: https://www.pnas.org/doi/full/10.1073/pnas.1916514117
    display: PNAS (2020)
  highlight: 0
  Parametrization: 1
 
  

- title: "Sodium channel slow inactivation normalizes firing in axons with uneven conductance distributions"
  image: Soduimchannelslowinactivation.JPG
  description: Axonal resilience
  authors: Zang Y. , Marder E. , Marom S.
  link:
    url: https://www.sciencedirect.com/science/article/pii/S0960982223003317?via%3Dihub
    display: Current Biology (2023)
  highlight: 0
  axonal: 1
 
  

- title: "A biophysical perspective on the resilience of neuronal excitability across timescales"
  image: Abiophysicalperpectiveonthe.png
  description: Resilience of neuronal excitability across time scalesNeuronal membrane excitability must be resilient to perturbations that can take place over timescales from milliseconds
  link:
    url: https://www.nature.com/articles/s41583-023-00730-9
    display:  Nature Reviews Neuroscience volume
  highlight: 3
  

- title: "Learning in Networks of Cortical Neurons"
  image: dummy.png
  description:  " "
  authors: Shahaf G. , Marom S. 
  link:
    url: https://www.jneurosci.org/content/21/22/8782
    display: Journal of Neuroscience
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 
  ERC: 0

- title: "A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks"
  image: StatisicalFrameworkPaper.JPG
  description: We frame Out Of Distribution (OOD) detection in DNNs as a statistical hypothesis testing problem. Tests generated within our proposed framework combine evidence from the entire network.
  authors: M. Haroush, T. Frostig, R. Heller, **D. Soudry**
  link:
    url: https://openreview.net/forum?id=Oy9WeuZD51
    display:   ICLR 2022 (2022)
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2: 
  ERC: 0

- title: "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability"
  image: dummy.png
  description:  " "
  authors:  A. Tamar, **D. Soudry**, E. Zisselman
  link:
    url: https://proceedings.mlr.press/v162/nacson22a/nacson22a.pdf
    display: AAAI 2022 (15% acceptance rate)
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 
  ERC: 0

- title: "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks"
  image: AcceleratedSparsePaper.JPG
  description:  In this work, we first suggest a new measure called mask-diversity which correlates with the expected accuracy of the different types of structural pruning.
  authors: I. Hubara, B. Chmiel, M. Island, R. Banner, S. Naor, **D. Soudry**
  link:
    url: https://arxiv.org/abs/2102.08124
    display:  NeurIPS 2021 (2021)
  highlight: 1
  RefPro: 1
  JourPap: 0  
  RefAbs: 0
  Notes: 0
  news1: <a href="https://github.com/papers-submission/structured_transposable_masks"> See more details about this paper </a>

- title: "Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling"
  image: dummy.png
  description:  " "
  authors: N. Giladi, Z. Ben-Haim, S. Nevo, Y. Matias,**D. Soudry**
  link:
    url: https://arxiv.org/abs/2106.07218
    display:  NeurIPS 2021
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 
  ERC: 0

- title: "The Implicit Bias of Minima Stability: A View from Function Space"
  image: dummy.png
  description:  " "
  authors: R. Mulayoff, T. Michaeli, **D. Soudry**
  link:
    url: https://openreview.net/forum?id=2STmSnZAEt2
    display:  NeurIPS 2021
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 
  ERC: 0

- title:  "On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent"
  image: dummy.png
  description:  " "
  authors: S. Azulay, E. Moroshko, M. Shpigel Nacson, B. Woodworth, N. Srebro, A. Globerson, **D. Soudry**
  link:
    url: https://arxiv.org/abs/2102.09769
    display:  ICML 2021, Long talk (3% acceptance rate).
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 
  ERC: 0

- title: "Accurate Post Training Quantization With Small Calibration Sets"
  image: dummy.png
  description:  " "
  authors:  I. Hubara * , Y. Nahshan * , Y. Hanani*, R. Banner, **D. Soudry**
  link:
    url: https://arxiv.org/abs/2006.10518
    display:  ICML 2021
  highlight: 0
  RefPro: 1
  JourPap: 0  
  RefAbs: 0
  Notes: 0
  news1: 
  ERC: 0

  
- title: "Neural gradients are near-lognormal: understanding sparse and quantized training"
  image: NeuralGradientPaper.JPG
  description: We find that the distribution of neural gradients is approximately lognormal. Considering this, we suggest two closed-form analytical methods to reduce the computational and memory burdens of neural gradients.
  authors: B. Chmiel * , L. Ben-Uri * , M. Shkolnik, E. Hoffer, R. Banner, **D. Soudry**  
  link:
    url: https://openreview.net/forum?id=EoFNy62JGd
    display:  ICLR 2021 (2021)
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2:
  ERC: 0


- title: "Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy"
  image: ImplicitBiasinDeepPaper.JPG
  description: We provide a detailed asymptotic study of gradient flow trajectories and their implicit optimization bias when minimizing the exponential loss over "diagonal linear networks". This is the simplest model displaying a transition between "kernel" and non-kernel ("rich" or "active") regimes.
  authors:  E. Moroshko, S. Gunasekar, B. Woodworth, J. D. Lee, N. Srebro, **D. Soudry**
  link:
    url: https://arxiv.org/abs/2007.06738
    display:  NeurIPS 2020, Spotlight (3% acceptance rate) (2020)
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2:
  ERC: 0

- title: "Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization?"
  image: dummy.png
  description:  " "
  authors: Y. Blumenfeld, D. Gilboa, **D. Soudry**
  link:
    url: https://arxiv.org/abs/2007.01038
    display:  ICML 2020
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Kernel and Rich Regimes in Overparametrized Models"
  image: dummy.png
  description:   " "
  authors: B. Woodworth, S. Gunasekar,  P. Savarese, E. Moroshko, I. Golan, J. Lee, **D. Soudry**, N. Srebro
  link:
    url: https://arxiv.org/abs/2002.09277
    display:  COLT 2020
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "The Knowledge Within: Methods for Data-Free Model Compression"
  image: TheKnowledgeWithinPaper.JPG
  authors:  M. Haroush, I. Hubara, E. Hoffer, **D. Soudry**
  description: Recently, an extensive amount of research has been focused on compressing and accelerating Deep Neural Networks (DNN). So far, high compression rate algorithms require part of the training dataset for a low precision calibration, or a fine-tuning process. However, this requirement is unacceptable when the data is unavailable or contains sensitive information, as in medical and biometric use-cases. We present three methods for generating synthetic samples from trained models.
  link:
    url: https://arxiv.org/abs/1912.01274
    display:   CVPR 2020 
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2:


- title: "Augment Your Batch: Improving Generalization Through Instance Repetition"
  image: dummy.png
  description:  " "
  authors: E. Hoffer, T. Ben-Nun, N. Giladi, I. Hubara, T. Hoefler, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1901.09335
    display:  CVPR 2020
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?"
  image: AtStabilityEdgePaper.JPG
  authors: N. Giladi *, M. Shpigel Nacson *, E. Hoffer, **D. Soudry** 
  description: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same.
  link:
    url: https://arxiv.org/abs/1909.12340
    display:  ICLR 2020 
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2:


- title: "A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case"
  image: dummy.png
  description:  " "
  authors: G. Ongie, R. Willett, **D. Soudry**, N. Srebro
  link:
    url: https://arxiv.org/pdf/1910.01635.pdf
    display:  ICLR 2020 
  highlight: 0
  RefPro: 1
  JourPap: 0  
  RefAbs: 0
  Notes: 0
  news1: <a href="https://www.youtube.com/watch?v=WObxWBLAdsA"> video about this paper </a>
  
  
- title: "A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off"
  image: MeanFieldPaper.JPG
  description: We apply mean-field techniques to networks with quantized activations in order to evaluate the degree to which quantization degrades signal propagation at initialization. We derive initialization schemes which maximize signal propagation in such networks and suggest why this is helpful for generalization.
  authors: Y. Blumenfeld, D. Gilboa, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1906.00771
    display:  NeurIPS 2019
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2:

- title: "Post-training 4-bit quantization of convolution networks for rapid-deployment"
  image: PostTrainingPaper.JPG
  description: This paper introduces the first practical 4-bit post training quantization approach.
  authors: R. Banner, Y. Nahshan, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1810.05723
    display:   NeurIPS 2019 
  highlight: 1
  RefPro: 1
  JourPap: 0  
  RefAbs: 0
  Notes: 0
  news1:  <a href = "https://github.com/submission2019/cnn-quantization"> See more details about this paper </a> 

- title: "Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models"
  image: dummy.png
  description:  " "
  authors: M. Shpigel Nacson, S. Gunasekar, J. Lee, N. Srebro, **D. Soudry**
  link:
    url: https://arxiv.org/pdf/1905.07325.pdf
    display:  ICML 2019 
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 
 

- title: "How do infinite width bounded norm networks look in function space?"
  image: HowToInfinitePaper.JPG
  description: We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded.  
  authors: P. Savarese, I. Evron, **D. Soudry**, N. Srebro
  link:
    url: https://arxiv.org/abs/1902.05040
    display: COLT 2019 
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2: 

- title: "Convergence of Gradient Descent on Separable Data"
  image: dummy.png
  description:  " "
  authors: M. Shpigel Nacson, J. Lee, S. Gunasekar, N. Srebro, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1803.01905
    display:  AISTATS 2019, Oral Presentation (2.5% acceptance rate). 
  highlight: 0
  RefPro: 1
  JourPap: 0  
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate"
  image: dummy.png
  description:  " "
  authors: M. Shpigel Nacson, N. Srebro, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1806.01796v3
    display:  AISTATS 2019 
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Norm matters: efficient and accurate normalization schemes in deep networks"
  image: dummy.png
  description:  " "
  authors: E. Hoffer * , R. Banner * , I. Golan * , **D. Soudry**
  link:
    url: https://proceedings.neurips.cc/paper_files/paper/2018/file/a0160709701140704575d499c997b6ca-Paper.pdf
    display:   NeurIPS 2018, Spotlight (3.5% acceptance rate)
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Implicit Bias of Gradient Descent on Linear Convolutional Networks"
  image: dummy.png
  description:  " "
  authors: S. Gunasekar, J. D. Lee, **D. Soudry**, N. Srebro
  link:
    url:  https://arxiv.org/abs/1806.00468
    display:   NeurIPS 2018
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

  
- title: "Scalable Methods for 8-bit Training of Neural Networks"
  image: ScalableMethodPaper.JPG
  description: Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision. Armed with this knowledge, we quantize the model parameters, activations and layer gradients to 8-bit, leaving at a higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity.
  authors: R. Banner, I. Hubara, E. Hoffer, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1805.11046
    display:  NeurIPS 2018
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2:

- title: "Characterizing Implicit Bias in Terms of Optimization Geometry"
  image: dummy.png
  description:  " "
  authors: S. Gunasekar, J. Lee, **D. Soudry**, N. Srebro
  link:
    url:  https://arxiv.org/abs/1802.08246
    display:   ICML 2018
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "The Implicit Bias of Gradient Descent on Separable Data"
  image: TheImplicitBiasofGradientDescent.JPG
  description: We show that gradient descent on an unregularized logistic regression problem, for almost all separable datasets, converges to the same direction as the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself.
  authors: <b> D. Soudry </b>, E. Hoffer, M. Shpigel Nacson, N. Srebro
  link:
    url: https://openreview.net/pdf?id=r1q7n9gAb
    display: ICLR 2018 (2018)
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news2:

- title: "Fix your classifier: the marginal value of training the last weight layer"
  image: dummy.png
  description:  " "
  authors: E. Hoffer, I. Hubara, **D. Soudry**
  link:
    url:  https://arxiv.org/abs/1801.04540
    display:   ICLR 2018
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 


- title: "Train longer, generalize better: closing the generalization gap in large batch training of neural networks"
  image: dummy.png
  description:  " "
  authors: E. Hoffer * , I. Hubara * , **D. Soudry**
  link:
    url:  https://papers.nips.cc/paper_files/paper/2017/hash/a5e0ff62be0b08456fc7f1e88812af3d-Abstract.html
    display:   NIPS 2017, Oral presentation (1.2% acceptance rate)
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: <a href = "https://github.com/eladhoffer/bigBatch"> See more details about this paper </a> 
  
- title: "Binarized Neural Networks"
  image: BinarizedNeuralNetworksPaper.JPG
  description: We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. 
  authors: I. Hubara *, M. Courbariaux *, **D. Soudry**, R. El-Yaniv, Y. Bengio
  link:
    url: https://arxiv.org/abs/1602.02830
    display:  NIPS 2016 (2016)
  highlight: 1
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 
  news2: 

- title: "A Fully Analog Memristor-Based Multilayer Neural Network with Online Backpropagation Training"
  image: dummy.png
  description:  " "
  authors: S. Greshnikov, E. Rosenthal, **D. Soudry**, and S. Kvatinsky
  link:
    url:  https://www.semanticscholar.org/paper/A-fully-analog-memristor-based-neural-network-with-Rosenthal-Greshnikov/66879965418dedeadadd1761b386a1dbaa67c7b2
    display:  Proceeding of the IEEE International Conference on Circuits and Systems, pp. 1394-1397, 2016
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous Or Discrete Weights"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b>, I. Hubara and R. Meir
  link:
    url:  https://proceedings.neurips.cc/paper_files/paper/2014/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf
    display:  NIPS 2014
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: <a href = "https://www.dropbox.com/s/i1aech4zd78ot3a/paper_with_appendix.pdf?dl=0"> Paper with appendix (with a few typos corrected from NIPS website) </a>  , <a href = "https://github.com/ExpectationBackpropagation/EBP_Matlab_Code/"> Code </a> ,<a href = "https://arxiv.org/abs/1503.03562"> Preliminary results on MNIST </a> 

- title: "Neuronal spike generation mechanism as an oversampling, noise-shaping A-to-D converter"
  image: dummy.png
  description:  " "
  authors:  D. B. Chklovskii and **D. Soudry**
  link:
    url:  https://papers.nips.cc/paper_files/paper/2012/hash/36660e59856b4de58a219bcf4e27eba3-Abstract.html
    display:  NIPS, 2012
  highlight: 0
  RefPro: 1
  JourPap: 0 
  RefAbs: 0
  Notes: 0
  news1: 








- title: "Training of Quantized Deep Neural Networks using a Magnetic Tunnel Junction-Based Synapse"
  image: dummy.png
  description:  " "
  authors:  T. Greenberg-Toledo, B. Perach, I. Hubara, **D. Soudry**, S. Kvatinsky
  link:
    url:  https://iopscience.iop.org/article/10.1088/1361-6641/ac251b/meta
    display:  to appear in  Semiconductor Science and Technology, 2021
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 


- title: "Task Agnostic Continual Learning Using Online Variational Bayes with Fixed-Point Updates" 
  image: dummy.png
  description:  " "
  authors:   C. Zeno *, I. Golan *, E. Hoffer, **D. Soudry**
  link:
    url: https://direct.mit.edu/neco/article/33/11/3139/107073/Task-Agnostic-Continual-Learning-Using-Online
    display:  Neural Computation, 2021
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "The Global Optimization Geometry of Shallow Linear Neural Networks" 
  image: dummy.png
  description:  " "
  authors:    Z. Zhu, **D. Soudry**, Y. C. Eldar, M. B. Wakin
  link:
    url: https://link.springer.com/article/10.1007/s10851-019-00889-w?wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst&utm_source=ArticleAuthorContributingOnlineFirst&utm_medium=email&utm_content=AA_en_06082018&ArticleAuthorContributingOnlineFirst_20190603
    display:  Journal of Mathematical Imaging and Vision, 2019
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Seizure pathways: A model-based investigation" 
  image: dummy.png
  description:  " "
  authors:    P. J. Karoly, L. Kuhlmann, **D. Soudry**, D. B. Grayden, M. J. Cook, D. R. Freestone
  link:
    url: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006403
    display: PLoS Comput Biol., vol. 14 no. 10, e1006403, 2018
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "The Implicit Bias of Gradient Descent on Separable Data" 
  image: dummy.png
  description:  " "
  authors:     <b> D. Soudry </b>, E. Hoffer, M. Shpigel Nacson, S. Gunasekar, N. Srebro
  link:
    url: https://jmlr.org/papers/v19/18-188.html
    display: JMLR, 2018
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Bifurcation Analysis of Two Coupled Jansen-Rit Neural Mass Models"
  image: dummy.png
  description:  " "
  authors: S. Ahmadizadeh, P. Jane Karoly, D. Nesic, D. Br. Grayden, M. J.Cook, **D. Soudry**, D. R. Freestone
  link:
    url: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192842
    display: PLOS One, vol. 13 no. 3, e0192842, 2018
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"
  image: dummy.png
  description:  " "
  authors:  I. Hubara * , M. Courbariaux * ,**D. Soudry**, R. El-Yaniv, Y. Bengio. 
  link:
    url: https://jmlr.org/papers/v18/16-456.html
    display: JMLR, 2018
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Multi-scale approaches for high-speed imaging and analysis of large neural populations"
  image: dummy.png
  description:  " "
  authors:   J. Friedrich, W. Yang, **D. Soudry**, Y. Mu, M. B. Ahrens, R. Yuste, D. S. Peterka, L. Paninski 
  link:
    url: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005685
    display: PLos Comput Biol, vol., 13 no. 8, e1005685, 2017
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis"
  image: dummy.png
  description:  " "
  authors:    Y. Dordek *, D. Soudry *, R. Meir, D. Derdikman
  link:
    url: https://elifesciences.org/articles/10094v2
    display: eLife, vol. 5, e10094, 2016
  news1: <a href="https://elifesciences.org/articles/10094v2"> F1000 Recommended </a>
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0


- title: "Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data"
  image: dummy.png
  description:  " "
  authors:  E. A. Pnevmatikakis, **D. Soudry**, Y. Gao, T. A. Machado, J. Merel, D. Pfau,T. Reardon,Y. Mu, C. Lacefield, W. Yang, M. Ahrens, R. Bruno, T. M. Jessell, D. S. Peterka, R. Yuste, L. Paninski,
  link:
    url: https://www.cell.com/neuron/fulltext/S0896-6273(15)01084-3
    display: Neuron, vol. 89, no. 2,  2016
  news1: <a href="https://www.cell.com/cms/attachment/2046949148/2057837139/mmc1.pdf"> ِAppendix </a> , <a href="https://github.com/flatironinstitute/CaImAn-MATLAB">Code </a>
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0


- title: "Efficient 'Shotgun' Inference of Neural Connectivity from Highly Sub-sampled Activity Data"
  image: dummy.png
  description:  " "
  authors:   D. Soudry, S. Keshri, P. Stinson, M.H. Oh, G. Iyengar, L. Paninski
  link:
    url: https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1004464
    display:  PLoS Comput Biol, vol. 11, no. 10, 2015
  news1: <a href="https://videolectures.net/netadis2015_soudry_activity_data/"> 5 mins presentation </a> , <a href="https://github.com/danielso/Shotgun">Code </a>
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 

- title: "Memristor-based multilayer neural networks with online gradient descent training"
  image: dummy.png
  description:  " "
  authors:  <b> D. Soudry </b>, D. Di Castro, A. Gal, A. Kolodny, and S. Kvatinsky
  link:
    url: https://ieeexplore.ieee.org/document/7010034
    display:  IEEE TNNLS, vol. 26, no. 10, 2015
  news1: <a href="https://www.dropbox.com/sh/7je4fufb6d4c2sr/AAA7RgsIlsRu-lNN6imELqjDa?dl=0"> Complete Code </a> 
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
 
- title: "Diffusion approximation-based simulation of stochastic ion channels: which method to use?"
  image: dummy.png
  description:  " "
  authors: D. Pezo, **D. Soudry**, P. Orio
  link:
    url: https://www.frontiersin.org/articles/10.3389/fncom.2014.00139/full
    display:  Front. Comput. Neurosci., vol. 8, no. 139, 2014
  news1: 
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0

    
- title: "The neuronal response at extended timescales: a linearized spiking input-output relation"
  image: dummy.png
  description:  " "
  authors:   <b> D. Soudry </b> and R. Meir
  link:
    url: https://www.frontiersin.org/articles/10.3389/fncom.2014.00029/full
    display:  Front. Comput. Neurosci., vol. 8, no. 29, 2014
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 
       
- title: "The neuronal response at extended timescales: long term correlations without long memory"
  image: dummy.png
  description:  " "
  authors:   <b> D. Soudry </b> and R. Meir
  link:
    url: https://www.frontiersin.org/articles/10.3389/fncom.2014.00029/full
    display:  Front. Comput. Neurosci., vol. 8, no. 35, 2014
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1:  

- title: "Simple, fast and accurate implementation of the diffusion approximation algorithm for stochastic ion channels with multiple states"
  image: dummy.png
  description:  " "
  authors: P. Orio and **D. Soudry**
  link:
    url: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0036670
    display: PLoS ONE, vol. 7, no. 5 p. e36670, 2012
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 


- title: "Conductance-based neuron models and the slow dynamics of excitability"
  image: dummy.png
  description:  " "
  authors:  <b> D. Soudry </b> and R. Meir
  link:
    url: https://www.frontiersin.org/articles/10.3389/fncom.2012.00004/full
    display: Front. Comput. Neurosci., vol. 6, no. 4, 2012
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 
 
- title: "History-Dependent Dynamics in a Generic Model of Ion Channels–An Analytic Study"
  image: dummy.png
  description:  " "
  authors:  <b> D. Soudry </b> and R. Meir
  link:
    url: https://www.frontiersin.org/articles/10.3389/fncom.2010.00003/full
    display: Front. Comput. Neurosci., vol. 4, Jan. 2010
  highlight: 0
  RefPro: 0
  JourPap: 1 
  RefAbs: 0
  Notes: 0
  news1: 





















  

- title: "Why Cold Posteriors? On the Suboptimal Generalization of Optimal Bayes Estimates"
  image: dummy.png
  description:  " "
  authors: C. Zeno, I. Golan, A. Pakman, **D. Soudry**
  link:
    url: https://openreview.net/forum?id=cu6zDHCfhZx
    display: Third Symposium on Advances in Approximate Bayesian Inference, contributed talk (2021).
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "How Learning Rate and Delay Affect Minima Selection in Asynchronous Training of Neural Networks: Toward Closing the Generalization Gap (Oral)"
  image: dummy.png
  description:  " "
  authors: N. Giladi * , Mor Shpigel * , E. Hoffer, **D. Soudry**
  link:
    url: https://drive.google.com/file/d/101yxxakquNQYtr5CD7bdbDgDLVmt1H-J/view
    display: ICML 'Understanding and Improving Generalization in Deep Learning' workshop (2019)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 
 
- title: "A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off (Oral)"
  image: dummy.png
  description:  " "
  authors:  Y. Blumenfeld, D. Gilboa, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1906.00771
    display:  ICML 'Physics for deep learning' workshop (2019)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 
  
- title: "Increasing batch size through instance repetition improves generalization (Poster)"
  image: dummy.png
  description:  " "
  authors:  E. Hoffer, T. Ben-Nun, N. Giladi, I. Hubara, T. Hoefler, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1901.09335
    display: ICML 'Understanding and Improving Generalization in Deep Learning' workshop, poster (2019)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 
  

- title: "Task Agnostic Continual Learning Using Online Variational Bayes (Poster)"
  image: dummy.png
  description:  " "
  authors:  C. Zeno *, I. Golan *, E. Hoffer, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1803.10123
    display:  NIPS Deep Bayesian learning workshop, 2018
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Infer2Train: leveraging inference for better training of deep networks (Poster)"
  image: dummy.png
  description:  " "
  authors:   E Hoffer, B Weinstein, I Hubara, S Gofman, **D Soudry**
  link:
    url: http://learningsys.org/nips18/assets/papers/24CameraReadySubmissionInfer2Train.pdf
    display: NIPS Deep Bayesian learning workshop, 2018
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Exponentially vanishing sub-optimal local minima in multilayer neural networks (Poster)"
  image: dummy.png
  description:  " "
  authors:  D. Soudry, E. Hoffer
  link:
    url: https://arxiv.org/abs/1702.05777
    display: ICLR workshop, 2018
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Quantized Neural Networks (Poster)"
  image: dummy.png
  description:  " "
  authors: I. Hubara * , M. Courbariaux  *, **D. Soudry**, R. El-Yaniv, Y. Bengio
  link:
    url: 
    display: NIPS workshop on Efficient Methods for Deep Neural Networks  (2016)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Quantized Neural Networks (Poster)"
  image: dummy.png
  description:  " "
  authors: I. Hubara * , M. Courbariaux  *, **D. Soudry**, R. El-Yaniv, Y. Bengio
  link:
    url: 
    display: NIPS workshop on Efficient Methods for Deep Neural Networks  (2016)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Binarized neural networks (Poster)"
  image: dummy.png
  description:  " "
  authors: I. Hubara * , M. Courbariaux  *, **D. Soudry**, R. El-Yaniv, Y. Bengio
  link:
    url: 
    display: Machine Learning seminar, IBM research center, Haifa (2016)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Data-driven neural models part II: connectivity patterns of human seizures (Best student poster)"
  image: dummy.png
  description:  " "
  authors: P. J. Karoly, D. R. Freestone, **D. Soudry**, L. Kuhlmann, L. Paninski, M. Cook
  link:
    url: 
    display: CNS (2016)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Data-driven neural models part I: state and parameter estimation (Poster)"
  image: dummy.png
  description:  " "
  authors: D. R. Freestone, P. J. Karoly, **D. Soudry**, L. Kuhlmann, M.Cook
  link:
    url: 
    display: CNS (2016)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Extracting grid characteristics from spatially distributed place cell inputs using non-negative PCA (Poster)"
  image: dummy.png
  description:  " "
  authors: Y. Dordek * , <b> D. Soudry </b> * , R. Meir,  D. Derdikman
  link:
    url: 
    display: SFN (2015)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Fast Constrained Non-negative Matrix Factorization for Whole-Brain Calcium Imaging Data (Poster)"
  image: dummy.png
  description:  " "
  authors: J. Friedrich, **D. Soudry**, Y. Mu, J. Freeman, M. Ahrens, and L. Paninski
  link:
    url: https://users.soe.ucsc.edu/~afletcher/neuralsysnips.html
    display: NIPS workshop on Statistical Methods for Understanding Neural Systems (2015)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Implementing efficient 'shotgun' inference of neural connectivity from highly sub-sampled activity data (Spotlight Presentation and poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b>, S. Keshri, P. Stinson, M.H. Oh, G. Iyengar, L. Paninski
  link:
    url: https://videolectures.net/netadis2015_soudry_activity_data/
    display: NIPS workshop on Modelling and Inference for Dynamics on Complex Interaction Networks (2015)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous Or Discrete Weights (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b>,I. Hubara and R. Meir
  link:
    url: 
    display: Machine Learning seminar (IBM research center, Haifa 2015)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: Efficient “shotgun” inference of neural connectivity from highly sub-sampled activity data (Oral)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b>, S. Keshri, P. Stinson, M.H. Oh, G. Iyengar, L. Paninski
  link:
    url: 
    display:  Swartz Annual Meeting at Janelia Research Campus (2015)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 
  
- title: "A shotgun sampling solution for the common input problem in neural connectivity inference (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry  </b>, S. Keshri, P. Stinson, M.H. Oh, G. Iyengar, L. Paninski
  link:
    url: 
    display:  COSYNE (2015)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Whole Brain Region of Interest Detection (Poster)"
  image: dummy.png
  description:  " "
  authors:  D. Pfau *,  <b>D. Soudry </b> *, Y. Gao, Y. Mu, J. Freeman, M. Ahrens, L. Paninski 
  link:
    url: 
    display: NIPS workshop on Large scale optical physiology -From data-acquisition to models of neural coding (2014) 
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "Whole Brain Region of Interest Detection (Poster)"
  image: dummy.png
  description:  " "
  authors:  D. Pfau *, <b> D. Soudry  </b>*, Y. Gao, Y. Mu, J. Freeman, M. Ahrens, L. Paninski 
  link:
    url: 
    display:  AREADNE (2014)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: " Mean Field Bayes Backpropagation: scalable training of multilayer neural networks with discrete weights (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b>and R. Meir
  link:
    url: 
    display:  Machine Learning seminar (IBM research center, Haifa 2013)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 


- title: "Implementing Hebbian Learning Rules with Memristors (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudr </b>y, D. Di Castro, A. Gal, A. Kolodny, and S. Kvatinsky
  link:
    url: 
    display: Memristor-based Systems for Neuromorphic Applications (Torino University 2013)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "A spiking input-output relation for general biophysical neuron models explains observed 1/f response (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b> and R. Meir
  link:
    url: 
    display:  COSYNE (2013)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 


- title: "Spiking input-output relation of general biophysical neuron models – exact analytic solutions and comparisons with experiment (Poster)"
  image: dummy.png
  description:  " "
  authors: <b>  D. Soudry </b> and R. Meir
  link:
    url: 
    display:  Variants and invariants in brain and behavior (Technion 2012)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "The slow dynamics of neuronal excitability - exact analytic solutions for the response of general biophysical neuron models at long times, and comparisons with experiment (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b> and R. Meir
  link:
    url: 
    display:  Brain Plasticity Symposium (Tel Aviv university 2012)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "The slow dynamics of neuronal excitability (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b> and R. Meir
  link:
    url: 
    display:  ISFN (2012)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "The neuron as a population of ion channels: The emergence of stochastic and history dependent behavior (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b> and R. Meir
  link:
    url: 
    display:  COSYNE (2011)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

- title: "The neuron as a population of ion channels: The emergence of stochastic and history dependent behavior (Oral)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b> and R. Meir
  link:
    url: 
    display:  ISFN (2010)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 

  
- title: "History dependent dynamics in ion channels - an analytic study (Poster)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b> and R. Meir
  link:
    url: 
    display:  COSYNE (2010)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 
  
- title: "Adapting Timescales: From Channel to Neuron” (Oral)"
  image: dummy.png
  description:  " "
  authors: <b> D. Soudry </b> and R. Meir
  link:
    url: 
    display:  ISFN (2009)
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 1
  Notes: 0
  news1: 







- title: "Mix & Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency"
  image: dummy.png
  description:  " "
  authors:  E. Hoffer, B. Weinstein, I. Hubara, T. Ben-Nun, T. Hoefler, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1908.08986
    display:  See Here
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 0
  Notes: 1
  news1: 

- title: "On the Blindspots of Convolutional Networks"
  image: dummy.png
  description:  " "
  authors:   E. Hoffer, S. Fine, **D. Soudry**
  link:
    url: https://arxiv.org/abs/1802.05187
    display:  See Here
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 0
  Notes: 1
  news1: 

- title: "No bad local minima: Data independent training error guarantees for multilayer neural networks"
  image: dummy.png
  description:  " "
  authors:  <b> D. Soudry </b>, E. Hoffer
  link:
    url: https://arxiv.org/abs/1605.08361
    display:  See Here
  highlight: 0
  RefPro: 0
  JourPap: 0 
  RefAbs: 0
  Notes: 1
  news1: 
